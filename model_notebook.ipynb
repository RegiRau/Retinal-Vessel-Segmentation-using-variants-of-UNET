{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "#convolutional block\n",
    "def conv_block(x, kernelsize, filters, dropout, batchnorm=False): \n",
    "    conv = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding=\"same\")(x)\n",
    "    if batchnorm is True:\n",
    "        conv = layers.BatchNormalization(axis=3)(conv)\n",
    "    conv = layers.Activation(\"relu\")(conv)\n",
    "    if dropout > 0:\n",
    "        conv = layers.Dropout(dropout)(conv)\n",
    "    conv = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding=\"same\")(conv)\n",
    "    if batchnorm is True:\n",
    "        conv = layers.BatchNormalization(axis=3)(conv)\n",
    "    conv = layers.Activation(\"relu\")(conv)\n",
    "    return conv\n",
    "\n",
    "\n",
    "#residual convolutional block\n",
    "def res_conv_block(x, kernelsize, filters, dropout, batchnorm=False):\n",
    "    conv1 = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding='same')(x)\n",
    "    if batchnorm is True:\n",
    "        conv1 = layers.BatchNormalization(axis=3)(conv1)\n",
    "    conv1 = layers.Activation('relu')(conv1)    \n",
    "    conv2 = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding='same')(conv1)\n",
    "    if batchnorm is True:\n",
    "        conv2 = layers.BatchNormalization(axis=3)(conv2)\n",
    "        conv2 = layers.Activation(\"relu\")(conv2)\n",
    "    if dropout > 0:\n",
    "        conv2 = layers.Dropout(dropout)(conv2)\n",
    "        \n",
    "    #skip connection    \n",
    "    shortcut = layers.Conv2D(filters, kernel_size=(1, 1), kernel_initializer='he_normal', padding='same')(x)\n",
    "    if batchnorm is True:\n",
    "        shortcut = layers.BatchNormalization(axis=3)(shortcut)\n",
    "    shortcut = layers.Activation(\"relu\")(shortcut)\n",
    "    respath = layers.add([shortcut, conv2])       \n",
    "    return respath\n",
    "\n",
    "\n",
    "#gating signal for attention unit\n",
    "def gatingsignal(input, out_size, batchnorm=False):\n",
    "    x = layers.Conv2D(out_size, (1, 1), padding='same')(input)\n",
    "    if batchnorm:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "#attention unit/block based on soft attention\n",
    "def attention_block(x, gating, inter_shape):\n",
    "    shape_x = K.int_shape(x)\n",
    "    shape_g = K.int_shape(gating)\n",
    "    theta_x = layers.Conv2D(inter_shape, (2, 2), strides=(2, 2), kernel_initializer='he_normal', padding='same')(x) \n",
    "    shape_theta_x = K.int_shape(theta_x)\n",
    "    phi_g = layers.Conv2D(inter_shape, (1, 1), kernel_initializer='he_normal', padding='same')(gating)\n",
    "    upsample_g = layers.Conv2DTranspose(inter_shape, (3, 3), strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]), kernel_initializer='he_normal', padding='same')(phi_g)\n",
    "    concat_xg = layers.add([upsample_g, theta_x])\n",
    "    act_xg = layers.Activation('relu')(concat_xg)\n",
    "    psi = layers.Conv2D(1, (1, 1), kernel_initializer='he_normal', padding='same')(act_xg)\n",
    "    sigmoid_xg = layers.Activation('sigmoid')(psi)\n",
    "    shape_sigmoid = K.int_shape(sigmoid_xg)\n",
    "    upsample_psi = layers.UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg) \n",
    "    upsample_psi = layers.Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3), arguments={'repnum': shape_x[3]})(upsample_psi)                          \n",
    "    y = layers.multiply([upsample_psi, x])\n",
    "    result = layers.Conv2D(shape_x[3], (1, 1), kernel_initializer='he_normal', padding='same')(y)\n",
    "    attenblock = layers.BatchNormalization()(result)\n",
    "    return attenblock\n",
    "\n",
    "#Simple U-NET\n",
    "def unetmodel(input_shape, dropout=0.2, batchnorm=True):    \n",
    "    \n",
    "    filters = [16, 32, 64, 128, 256]\n",
    "    kernelsize = 3\n",
    "    upsample_size = 2\n",
    "    \n",
    "    inputs = layers.Input(input_shape)    \n",
    "\n",
    "    # Downsampling layers\n",
    "    dn_1 = conv_block(inputs, kernelsize, filters[0], dropout, batchnorm)\n",
    "    pool_1 = layers.MaxPooling2D(pool_size=(2,2))(dn_1)\n",
    "    \n",
    "    dn_2 = conv_block(pool_1, kernelsize, filters[1], dropout, batchnorm)\n",
    "    pool_2 = layers.MaxPooling2D(pool_size=(2,2))(dn_2)\n",
    "    \n",
    "    dn_3 = conv_block(pool_2, kernelsize, filters[2], dropout, batchnorm)\n",
    "    pool_3 = layers.MaxPooling2D(pool_size=(2,2))(dn_3)\n",
    "    \n",
    "    dn_4 = conv_block(pool_3, kernelsize, filters[3], dropout, batchnorm)\n",
    "    pool_4 = layers.MaxPooling2D(pool_size=(2,2))(dn_4)\n",
    "    \n",
    "    dn_5 = conv_block(pool_4, kernelsize, filters[4], dropout, batchnorm)\n",
    "\n",
    "    # Upsampling layers   \n",
    "    up_5 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(dn_5)\n",
    "    up_5 = layers.concatenate([up_5, dn_4], axis=3)\n",
    "    up_conv_5 = conv_block(up_5, kernelsize, filters[3], dropout, batchnorm)\n",
    "    \n",
    "    up_4 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(up_conv_5)\n",
    "    up_4 = layers.concatenate([up_4, dn_3], axis=3)\n",
    "    up_conv_4 = conv_block(up_4, kernelsize, filters[2], dropout, batchnorm)\n",
    "       \n",
    "    up_3 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(up_conv_4)\n",
    "    up_3 = layers.concatenate([up_3, dn_2], axis=3)\n",
    "    up_conv_3 = conv_block(up_3, kernelsize, filters[1], dropout, batchnorm)\n",
    "    \n",
    "    up_2 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(up_conv_3)\n",
    "    up_2 = layers.concatenate([up_2, dn_1], axis=3)\n",
    "    up_conv_2 = conv_block(up_2, kernelsize, filters[0], dropout, batchnorm)    \n",
    "   \n",
    "    conv_final = layers.Conv2D(1, kernel_size=(1,1))(up_conv_2)\n",
    "    conv_final = layers.BatchNormalization(axis=3)(conv_final)\n",
    "    outputs = layers.Activation('sigmoid')(conv_final)  \n",
    "\n",
    "    model = models.Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.summary()        \n",
    "    return model\n",
    "\n",
    "    \n",
    "#Attention U-NET\n",
    "def attentionunet(input_shape, dropout=0.2, batchnorm=True):\n",
    "    \n",
    "    filters = [16, 32, 64, 128, 256]\n",
    "    kernelsize = 3\n",
    "    upsample_size = 2\n",
    "\n",
    "    inputs = layers.Input(input_shape) \n",
    "\n",
    "    # Downsampling layers    \n",
    "    dn_1 = conv_block(inputs, kernelsize, filters[0], dropout, batchnorm)\n",
    "    pool_1 = layers.MaxPooling2D(pool_size=(2,2))(dn_1)\n",
    "    \n",
    "    dn_2 = conv_block(pool_1, kernelsize, filters[1], dropout, batchnorm)\n",
    "    pool_2 = layers.MaxPooling2D(pool_size=(2,2))(dn_2)\n",
    "    \n",
    "    dn_3 = conv_block(pool_2, kernelsize, filters[2], dropout, batchnorm)\n",
    "    pool_3 = layers.MaxPooling2D(pool_size=(2,2))(dn_3)\n",
    "    \n",
    "    dn_4 = conv_block(pool_3, kernelsize, filters[3], dropout, batchnorm)\n",
    "    pool_4 = layers.MaxPooling2D(pool_size=(2,2))(dn_4)\n",
    "    \n",
    "    dn_5 = conv_block(pool_4, kernelsize, filters[4], dropout, batchnorm)\n",
    "\n",
    "    # Upsampling layers    \n",
    "    gating_5 = gatingsignal(dn_5, filters[3], batchnorm)\n",
    "    att_5 = attention_block(dn_4, gating_5, filters[3])\n",
    "    up_5 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(dn_5)\n",
    "    up_5 = layers.concatenate([up_5, att_5], axis=3)\n",
    "    up_conv_5 = conv_block(up_5, kernelsize, filters[3], dropout, batchnorm)\n",
    "    \n",
    "    gating_4 = gatingsignal(up_conv_5, filters[2], batchnorm)\n",
    "    att_4 = attention_block(dn_3, gating_4, filters[2])\n",
    "    up_4 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(up_conv_5)\n",
    "    up_4 = layers.concatenate([up_4, att_4], axis=3)\n",
    "    up_conv_4 = conv_block(up_4, kernelsize, filters[2], dropout, batchnorm)\n",
    "   \n",
    "    gating_3 = gatingsignal(up_conv_4, filters[1], batchnorm)\n",
    "    att_3 = attention_block(dn_2, gating_3, filters[1])\n",
    "    up_3 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(up_conv_4)\n",
    "    up_3 = layers.concatenate([up_3, att_3], axis=3)\n",
    "    up_conv_3 = conv_block(up_3, kernelsize, filters[1], dropout, batchnorm)\n",
    "    \n",
    "    gating_2 = gatingsignal(up_conv_3, filters[0], batchnorm)\n",
    "    att_2 = attention_block(dn_1, gating_2, filters[0])\n",
    "    up_2 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(up_conv_3)\n",
    "    up_2 = layers.concatenate([up_2, att_2], axis=3)\n",
    "    up_conv_2 = conv_block(up_2, kernelsize, filters[0], dropout, batchnorm)\n",
    "    \n",
    "    conv_final = layers.Conv2D(1, kernel_size=(1,1))(up_conv_2)\n",
    "    conv_final = layers.BatchNormalization(axis=3)(conv_final)\n",
    "    outputs = layers.Activation('sigmoid')(conv_final)  \n",
    "\n",
    "    model = models.Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.summary()       \n",
    "    return model    \n",
    "\n",
    "#Res-UNET\n",
    "def residualunet(input_shape, dropout=0.2, batchnorm=True):\n",
    "\n",
    "    filters = [16, 32, 64, 128, 256]\n",
    "    kernelsize = 3\n",
    "    upsample_size = 2\n",
    "\n",
    "    inputs = layers.Input(input_shape) \n",
    "\n",
    "    # Downsampling layers    \n",
    "    dn_conv1 = conv_block(inputs, kernelsize, filters[0], dropout, batchnorm)\n",
    "    dn_pool1 = layers.MaxPooling2D(pool_size=(2,2))(dn_conv1)\n",
    "\n",
    "    dn_conv2 = res_conv_block(dn_pool1, kernelsize, filters[1], dropout, batchnorm)\n",
    "    dn_pool2 = layers.MaxPooling2D(pool_size=(2,2))(dn_conv2)\n",
    "\n",
    "    dn_conv3 = res_conv_block(dn_pool2, kernelsize, filters[2], dropout, batchnorm)\n",
    "    dn_pool3 = layers.MaxPooling2D(pool_size=(2,2))(dn_conv3)\n",
    "\n",
    "    dn_conv4 = res_conv_block(dn_pool3, kernelsize, filters[3], dropout, batchnorm)\n",
    "    dn_pool4 = layers.MaxPooling2D(pool_size=(2,2))(dn_conv4)\n",
    "\n",
    "    dn_conv5 = res_conv_block(dn_pool4, kernelsize, filters[4], dropout, batchnorm)\n",
    "   \n",
    "    # upsampling layers\n",
    "    up_conv6 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(dn_conv5)\n",
    "    up_conv6 = layers.concatenate([up_conv6, dn_conv4], axis=3)\n",
    "    up_conv6 = res_conv_block(up_conv6, kernelsize, filters[3], dropout, batchnorm)\n",
    "\n",
    "    up_conv7 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(up_conv6)\n",
    "    up_conv7 = layers.concatenate([up_conv7, dn_conv3], axis=3)\n",
    "    up_conv7 = res_conv_block(up_conv7, kernelsize, filters[2], dropout, batchnorm)\n",
    "\n",
    "    up_conv8 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(up_conv7)\n",
    "    up_conv8 = layers.concatenate([up_conv8, dn_conv2], axis=3)\n",
    "    up_conv8 = res_conv_block(up_conv8, kernelsize, filters[1], dropout, batchnorm)\n",
    "\n",
    "    up_conv9 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(up_conv8)\n",
    "    up_conv9 = layers.concatenate([up_conv9, dn_conv1], axis=3)\n",
    "    up_conv9 = res_conv_block(up_conv9, kernelsize, filters[0], dropout, batchnorm)\n",
    "\n",
    "\n",
    "    conv_final = layers.Conv2D(1, kernel_size=(1,1))(up_conv9)\n",
    "    conv_final = layers.BatchNormalization(axis=3)(conv_final)\n",
    "    outputs = layers.Activation('sigmoid')(conv_final) \n",
    "    \n",
    "    model = models.Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "#Residual-Attention UNET (RA-UNET)\n",
    "def residual_attentionunet(input_shape, dropout=0.2, batchnorm=True):\n",
    "    \n",
    "    filters = [16, 32, 64, 128, 256]\n",
    "    kernelsize = 3\n",
    "    upsample_size = 2\n",
    "    \n",
    "    inputs = layers.Input(input_shape)    \n",
    "    \n",
    "    # Downsampling layers\n",
    "    dn_1 = res_conv_block(inputs, kernelsize, filters[0], dropout, batchnorm)\n",
    "    pool1 = layers.MaxPooling2D(pool_size=(2,2))(dn_1)\n",
    "\n",
    "    dn_2 = res_conv_block(pool1, kernelsize, filters[1], dropout, batchnorm)\n",
    "    pool2 = layers.MaxPooling2D(pool_size=(2,2))(dn_2)\n",
    "\n",
    "    dn_3 = res_conv_block(pool2, kernelsize, filters[2], dropout, batchnorm)\n",
    "    pool3 = layers.MaxPooling2D(pool_size=(2,2))(dn_3)\n",
    "\n",
    "    dn_4 = res_conv_block(pool3, kernelsize, filters[3], dropout, batchnorm)\n",
    "    pool4 = layers.MaxPooling2D(pool_size=(2,2))(dn_4)\n",
    "\n",
    "    dn_5 = res_conv_block(pool4, kernelsize, filters[4], dropout, batchnorm)\n",
    "\n",
    "    # Upsampling layers    \n",
    "    gating_5 = gatingsignal(dn_5, filters[3], batchnorm)\n",
    "    att_5 = attention_block(dn_4, gating_5, filters[3])\n",
    "    up_5 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(dn_5)\n",
    "    up_5 = layers.concatenate([up_5, att_5], axis=3)\n",
    "    up_conv_5 = res_conv_block(up_5, kernelsize, filters[3], dropout, batchnorm)\n",
    "    \n",
    "    gating_4 = gatingsignal(up_conv_5, filters[2], batchnorm)\n",
    "    att_4 = attention_block(dn_3, gating_4, filters[2])\n",
    "    up_4 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(up_conv_5)\n",
    "    up_4 = layers.concatenate([up_4, att_4], axis=3)\n",
    "    up_conv_4 = res_conv_block(up_4, kernelsize, filters[2], dropout, batchnorm)\n",
    "   \n",
    "    gating_3 = gatingsignal(up_conv_4, filters[1], batchnorm)\n",
    "    att_3 = attention_block(dn_2, gating_3, filters[1])\n",
    "    up_3 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(up_conv_4)\n",
    "    up_3 = layers.concatenate([up_3, att_3], axis=3)\n",
    "    up_conv_3 = res_conv_block(up_3, kernelsize, filters[1], dropout, batchnorm)\n",
    "    \n",
    "    gating_2 = gatingsignal(up_conv_3, filters[0], batchnorm)\n",
    "    att_2 = attention_block(dn_1, gating_2, filters[0])\n",
    "    up_2 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format=\"channels_last\")(up_conv_3)\n",
    "    up_2 = layers.concatenate([up_2, att_2], axis=3)\n",
    "    up_conv_2 = res_conv_block(up_2, kernelsize, filters[0], dropout, batchnorm)\n",
    "   \n",
    "    conv_final = layers.Conv2D(1, kernel_size=(1,1))(up_conv_2)\n",
    "    conv_final = layers.BatchNormalization(axis=3)(conv_final)\n",
    "    outputs = layers.Activation('sigmoid')(conv_final)  \n",
    "\n",
    "    model = models.Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.summary() \n",
    "    return model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}